---
title: "Machine Learning Project"
author: "Martin Halwachs"
date: "25 Mai 2017"
output:
  html_document:
    toc: TRUE
---

# requirements

- predict the `classe` variable  
- create report  
- describe how you **build the model**  
- describe how you use **cross validation**  
- estimate the **out of sample error**  
- explain why you made your choices  
- github repository  
    - R markdown file  
    - compiled HTM file  
    - less than 2000 words  
    - less than 5 figures  
    - submit with a gh-pages branch = allow online view of HTML  
    
for the quiz you will need to:

- predict 20 different test cases with your model  


# introduction
For this course project data will be used from http://groupware.les.inf.puc-rio.br/har. 6 participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Their activity was tracked by accelerometers on the belt, forearm, arm and dumbell. We will try to find a model, allowing to qualify a participant's quality of activity in terms of correctness.

# libraries and functions
This project needs the following libraries. In case you don't have them installed, you may use `install.packages()` function
```{r libraries, message=FALSE, warning=FALSE}
library(plyr)
library(dplyr)
library(tidyr)
library(caret)
library(rpart)
library(gbm)
library(survival)
library(splines)
library(parallel)
library(randomForest)
```

Further timing code chunks was inspired by https://stackoverflow.com/questions/24595280/timing-for-chunks

# Preparing the data set
First training and test data is loaded from the web and into `trainData` and `quizData`
```{r load data, message=FALSE, cache=TRUE}
if(!file.exists("training.csv"))
{
  trainURL <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  download.file(trainURL, destfile="training.csv", method="curl")
}
if(!file.exists("quiz.csv"))
{
  quizURL <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
  download.file(quizURL, destfile="quiz.csv", method="curl")
}
  
trainData <- read.table("training.csv", header=TRUE, sep=",", na.strings = "NA",
                        stringsAsFactors=FALSE)
quizData <- read.table("quiz.csv", header=TRUE, sep=",", na.strings = "NA",
                        stringsAsFactors=FALSE)
```

These data sets contain the following amount of data
```{r}
dim(trainData)
dim(quizData)
```

## filtering variables
These columns are in the training, but not in the testing set
```{r}
train.but.not.in.quiz <- names(trainData)[!names(trainData) %in% names(quizData)]
train.but.not.in.quiz
```

Therefore all data, but 'classe' are contained in the quiz set, while quiz contains the column `problem_id` (as column number of the data sets is equal). From all columns the ones with NAs in `quizData` can be excluded from model building, as they allow no prediction:
```{r}
nas.in.quizData <- apply(is.na(quizData), 2, sum)
nas.in.quizData <- nas.in.quizData[nas.in.quizData>0]
```

As the minimum number of NAs per variable is `r min(nas.in.quizData)` and the testing set only contains 20 rows, these can all be excluded from training and quiz data set.
```{r}
trainData <- trainData[,! names(trainData) %in% names(nas.in.quizData)]
quizData <- quizData[,! names(quizData) %in% names(nas.in.quizData)]
```

This leaves `r dim(trainData)` variables in `trainData` and `quizData` (including the predict)

Next `classe` and `user_name` are formatted to factors
```{r}
trainData$classe <- as.factor(trainData$classe)
trainData$user_name <- as.factor(trainData$user_name)
quizData$user_name <- as.factor(quizData$user_name)
```

## correcting the timestamp
the data contains two timestamps: `raw_timestamp_part_1` and `raw_timestamp_part_2`, besides the human readable `cvtd_timestamp`. Comparing `part_1` with the `cvtd`, we find, that it contains the main time
```{r}
as.POSIXct(head(trainData$raw_timestamp_part_1),origin="1970-01-01",tz="GMT")
head(trainData$cvtd_timestamp)
```

So what about `part_2`? This seems to be the milliseconds. Because, if we look at the first 44 elements, or at a plot versus data line number, we find a periodicity in a growth from 0 to 1,000,000 with following step to 0 again.
```{r, fig.width=12, figh.height=4}
head(trainData$raw_timestamp_part_2,n=44)
with(head(trainData,n=1000), plot(x=X, y=raw_timestamp_part_2,type="l"))
```

Therefore these 3 variables may be put into 1 timestamp column, this is demonstrated with `temp1` and `temp2` first.
```{r}
total.time <- function(part1, part2)
{
  as.POSIXct(part1,origin="1970-01-01",tz="GMT")+part2/1E6
}
temp1 <- as.POSIXct(head(trainData$raw_timestamp_part_1),origin="1970-01-01",tz="GMT")
temp2 <- total.time(head(trainData$raw_timestamp_part_1),head(trainData$raw_timestamp_part_2))
data.frame(just.part1=format(temp1, "%Y-%m-%d %H:%M:%OS6"),
           millis=head(trainData$raw_timestamp_part_2),
           total.time=format(temp2, "%Y-%m-%d %H:%M:%OS6"))
```

Neglecting minor rounding errors on the last digits, this is applied to training and quiz data set, deleting the other time columns. Further we dismiss the line number `X`
```{r}
trainData$timestamp <- total.time(trainData$raw_timestamp_part_1, trainData$raw_timestamp_part_2)
quizData$timestamp <- total.time(quizData$raw_timestamp_part_1, quizData$raw_timestamp_part_2)
trainData <- select(trainData, -c(raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, X))
quizData <- select(quizData, -c(raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, X))
```


## the tidy data set
Now the training data set has the following variables
```{r}
names(trainData)
```

# Data partitioning
For model building we split `trainData` in a set for `training` and a set for `testing`
```{r}
set.seed(1)
inTrain <- createDataPartition(y=trainData$classe, p=0.75, list=FALSE)
training <- trainData[inTrain,]
testing <- trainData[-inTrain,]
```

Now a model will be developed on the the training set
```{r}
dim(training)
```

# reduction of variables
First we try to find out about the importance of the time stamp for prediction:
```{r, fig.width=12, fig.height=5}
library(ggplot2)
p <- ggplot(filter(training, timestamp < "2011-11-30"), aes(x=timestamp, y=roll_belt, colour=classe))
p + geom_point()
```

From this plot, it is clear, the the instructions were confusing, and we don't need to predict whether a certaing activity was performed wrong or right. We need to predict, the activity performed. Therefore we can neglect the variables `timestamp`, `new_window` and `num_window` as they have a predefined correlation with `classe`, not allowing general prediction. Same goes for `user_name`
```{r}
training <- select(training, -c(timestamp, new_window, num_window, user_name))
```

Let's try to find highly correlated data with a simple method. A correlation matrix is calculated from all remaining columns, but `classe` and `user_name`. this can further be analyzed with the `findCorrelation` function
```{r}
corrMatrix <- cor(select(training,-c(classe)))
corrCols.names <- findCorrelation(corrMatrix, names=TRUE, cutoff=0.90)
corrCols <- as.numeric(lapply(corrCols.names, FUN=grep, x=names(training)))
corrCols.names
corrCols
```

These columns show a high correlation with some other column and are therefore removed
```{r}
dim(training)
training <- training[,-corrCols]
dim(training)
```


All remaining variables are then plotted against `classe` to reveal if logarithm of any variable is needed. Further this plot allows to compare all variables for their correlation with the desired predictor. As several plots contain outliers, not allowing to see the variability of each variable. So outliers were omitted.
```{r, fig.width=12, figh.height=20, cache=TRUE}
plotData <- filter(training, gyros_dumbbell_y<20 &
                             gyros_forearm_x>-15 &
                             gyros_forearm_y<100 &
                             gyros_forearm_z<100 &
                             magnet_dumbbell_y>-2000)
plotData <- gather(plotData, variable, value, -classe)
p <- ggplot(plotData, aes(x=value, y=classe, colour=classe))
p + geom_point(alpha=0.5) + facet_wrap(~variable, scale="free_x")
```

All data show good variability. An additional logarithm is not needed. From the above graphic, a **reduced set of variables** can be found. These show different dependences for `classe` and may therefore be the most useful variables for prediction.
```{r}
training.sub <- select(training, c(gyros_arm_y, gyros_belt_x, gyros_belt_y, gyros_belt_z,
                               gyros_dumbbell_y, gyros_forearm_x, gyros_forearm_z,
                               magnet_belt_x, magnet_belt_y, magnet_belt_z,
                               magnet_dumbbell_x, magnet_dumbbell_y, pitch_belt,
                               pitch_forearm, roll_arm, roll_forearm, total_accel_belt,
                               yaw_arm, yaw_belt, classe))
```

# Comparing models on the training set

From above, we have 2 sets of variables for training:  

* `training` which contains `r length(names(training)-1)` regressors  
* `training.sub` which has a reduced set of `r length(names(training.sub)-1)` regressors, compared to `training`  

To predict on `classe` the following model strategies will be compared:

* __Classification and regression trees__  
* __Adaboost__  
* __Random Forest__  

In order to apply __cross validation__ and have low computing times the following training control is used (with helpf from the caret documentation)
```{r}
fitControl <- trainControl(## 10-fold Cross validation
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 1) #25
```

## Classification and regression trees
First a simple model on `training` is created using **regression trees**
```{r simple model, cache=TRUE}
start.time <- Sys.time()
modFit.simple <- train(classe ~ ., data=training, method="rpart")
difftime(Sys.time(), start.time)
```

This results in an accuracy of
```{r}
predClasse <- predict(modFit.simple,training[,-grep("classe",names(training))])
table(predClasse, training$classe)
accuracy <- sum(predClasse == training$classe)/length(predClasse)
accuracy
```

Then the same method is applied to `training.sub`
```{r reduced model, cache=TRUE}
start.time <- Sys.time()
modFit.reduced <- train(classe ~ ., data=training.sub, method="rpart")
difftime(Sys.time(), start.time)
```

Leading to the following accuracy
```{r}
predClasse <- predict(modFit.reduced,training.sub[,-grep("classe",names(training.sub))])
table(predClasse, training.sub$classe)
accuracy <- sum(predClasse == training.sub$classe)/length(predClasse)
accuracy
```

## Adaboost
In the case of various varibles, with different correlations to the predictors, **adaboost** may be a useful method (using `gbm()` underneath). On `training` this results in:
```{r adaboost model, cache=TRUE}
start.time <- Sys.time()
modFit.ada <- train(classe ~ ., data=training, method="gbm", verbose=FALSE, trControl=fitControl)
difftime(Sys.time(), start.time)
```

with the following accuracy
```{r}
predClasse <- predict(modFit.ada,training[,-grep("classe",names(training))])
table(predClasse, training$classe)
accuracy <- sum(predClasse == training$classe)/length(predClasse)
accuracy
```

Using `training.sub` this results in
```{r adaboost model on reduced data, cache=TRUE}
start.time <- Sys.time()
modFit.ada.reduced <- train(classe ~ ., data=training.sub, method="gbm", verbose=FALSE, trControl=fitControl)
difftime(Sys.time(), start.time)
```

Acchieving an accuracy of
```{r}
predClasse <- predict(modFit.ada.reduced,training.sub[,-grep("classe",names(training.sub))])
table(predClasse, training.sub$classe)
accuracy <- sum(predClasse == training.sub$classe)/length(predClasse)
accuracy
```

## random forest
A more sophisticated method is **random forest**. This method is already expected to take much longer than the others. First this method is applied to `training`
```{r random forest model, cache=TRUE}
start.time <- Sys.time()
modFit.rf <- train(classe ~ ., data=training, method="rf", trControl=fitControl)
difftime(Sys.time(), start.time)
```

Leading to the following accuracy 
```{r}
predClasse <- predict(modFit.rf,training[,-grep("classe",names(training))])
table(predClasse, training$classe)
accuracy <- sum(predClasse == training$classe)/length(predClasse)
accuracy
```

Then the `training.sub` is used for training
```{r random forest model on reduced data, cache=TRUE}
start.time <- Sys.time()
modFit.rf.reduced <- train(classe ~ ., data=training.sub, method="rf", trControl=fitControl)
difftime(Sys.time(), start.time)
```

with an accuracy of 
```{r}
predClasse <- predict(modFit.rf.reduced,training.sub[,-grep("classe",names(training.sub))])
table(predClasse, training.sub$classe)
accuracy <- sum(predClasse == training.sub$classe)/length(predClasse)
accuracy
```

## summary
The above approaches show different accuracy, the "simple" method, using regression trees has the lowest and in fact too low accuracy, but high speed, taking just several seconds.

The adaboost aproach can be tuned, using cross validation and only 1 repetition, but still takes several minutes. Adaboost has higher accuracy and using the reduced variable set, doubles calculation speed and has only a minor drop in accuracy.

Random forest has best accuracy, but takes very long to compute, even for the cross validation setting with only 1 repetition.

Thus **adaboost** on the reduced variable set is chosen as compromise on speed and accuracy. To sum up the reduced data set contains the following variables:
```{r}
names(training.sub)
```

# validation
The chosen model is applied to the testing set to get the final accuracy and **out of sample error** (= 100% minus accuracy)
```{r}
predClasse <- predict(modFit.ada.reduced, testing)
table(predClasse, testing$classe)
accuracy <- sum(predClasse == testing$classe)/length(predClasse)
accuracy
```

The **out of sample error** is the remainder from accuracy to 100%
```{r}
out.of.sample.error <- 1 - sum(predClasse == testing$classe)/length(predClasse)
out.of.sample.error
```

Validation leads to:

* `r round(out.of.sample.error*100,2)`% of error and  
* an accuracy of `r round(100-out.of.sample.error*100,2)`%.

# Prediction on the quiz data
Finally this model is applied to the quiz data to predict `classe`
```{r}
predClasse <- predict(modFit.ada.reduced, quizData)
predClasse
```
